{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classification : Nearest Neighbors and Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Perform k-Nearest neighbours on the given dataset($X_{knn}$ and $y_{knn}$: where $X_{knn}$ stores feature vectors representing the movies and  $y_{knn}$ stores the 0-1 labelling for each movie) for binary classification of movies, for classifiying whether a given movie is a comedy(label 1) or not a comedy(label 0) . Split the dataset into train(80%), validation(10%) and test sets(10%).Run k-Nearest neighbours for different k values (1,3,7,15,31,63). Select the k, using validation set, which returns the best accuracy score. \n",
    "\n",
    "(i)  Report all the validation accuracies for all the values of k. \n",
    "<br>(ii) Report accuracy score by performing k-NN on the test dataset using the best chosen k value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write your code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"X_knn.csv\", sep = ' ', header=None)\n",
    "df2 = pd.read_csv(\"y_knn.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_df = list(df1[0])\n",
    "# split_col_list = [row.split() for row in list_df]\n",
    "# for i, row in enumerate(split_col_list):\n",
    "#     split_col_list[i] = [float(val) for val in split_col_list[i]]\n",
    "    \n",
    "#X = np.array(split_col_list)\n",
    "X = np.array(df1)\n",
    "y = np.array(df2)\n",
    "y = y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1118</th>\n",
       "      <th>1119</th>\n",
       "      <th>1120</th>\n",
       "      <th>1121</th>\n",
       "      <th>1122</th>\n",
       "      <th>1123</th>\n",
       "      <th>1124</th>\n",
       "      <th>1125</th>\n",
       "      <th>1126</th>\n",
       "      <th>1127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.02500</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>0.05775</td>\n",
       "      <td>0.09675</td>\n",
       "      <td>0.14675</td>\n",
       "      <td>0.21700</td>\n",
       "      <td>0.06700</td>\n",
       "      <td>0.26275</td>\n",
       "      <td>0.26200</td>\n",
       "      <td>0.03200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03950</td>\n",
       "      <td>0.01800</td>\n",
       "      <td>0.04575</td>\n",
       "      <td>0.03275</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.04150</td>\n",
       "      <td>0.01925</td>\n",
       "      <td>0.03625</td>\n",
       "      <td>0.07775</td>\n",
       "      <td>0.02300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.03975</td>\n",
       "      <td>0.04375</td>\n",
       "      <td>0.03775</td>\n",
       "      <td>0.04800</td>\n",
       "      <td>0.11025</td>\n",
       "      <td>0.07250</td>\n",
       "      <td>0.04775</td>\n",
       "      <td>0.10975</td>\n",
       "      <td>0.09925</td>\n",
       "      <td>0.02050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04175</td>\n",
       "      <td>0.01925</td>\n",
       "      <td>0.01725</td>\n",
       "      <td>0.02425</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.01475</td>\n",
       "      <td>0.09025</td>\n",
       "      <td>0.01875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.04350</td>\n",
       "      <td>0.05475</td>\n",
       "      <td>0.02800</td>\n",
       "      <td>0.07700</td>\n",
       "      <td>0.05400</td>\n",
       "      <td>0.06850</td>\n",
       "      <td>0.05600</td>\n",
       "      <td>0.18500</td>\n",
       "      <td>0.04925</td>\n",
       "      <td>0.02675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04150</td>\n",
       "      <td>0.02675</td>\n",
       "      <td>0.02775</td>\n",
       "      <td>0.03425</td>\n",
       "      <td>0.15550</td>\n",
       "      <td>0.03675</td>\n",
       "      <td>0.01700</td>\n",
       "      <td>0.01950</td>\n",
       "      <td>0.09700</td>\n",
       "      <td>0.01850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03725</td>\n",
       "      <td>0.03950</td>\n",
       "      <td>0.03675</td>\n",
       "      <td>0.03100</td>\n",
       "      <td>0.06825</td>\n",
       "      <td>0.04050</td>\n",
       "      <td>0.02325</td>\n",
       "      <td>0.08700</td>\n",
       "      <td>0.05125</td>\n",
       "      <td>0.03025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05750</td>\n",
       "      <td>0.03375</td>\n",
       "      <td>0.02275</td>\n",
       "      <td>0.03975</td>\n",
       "      <td>0.18525</td>\n",
       "      <td>0.05925</td>\n",
       "      <td>0.01500</td>\n",
       "      <td>0.01525</td>\n",
       "      <td>0.06450</td>\n",
       "      <td>0.01300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04200</td>\n",
       "      <td>0.05275</td>\n",
       "      <td>0.05925</td>\n",
       "      <td>0.03675</td>\n",
       "      <td>0.07525</td>\n",
       "      <td>0.12525</td>\n",
       "      <td>0.02850</td>\n",
       "      <td>0.08500</td>\n",
       "      <td>0.02950</td>\n",
       "      <td>0.02875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04250</td>\n",
       "      <td>0.02825</td>\n",
       "      <td>0.02150</td>\n",
       "      <td>0.02600</td>\n",
       "      <td>0.14275</td>\n",
       "      <td>0.02075</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.01675</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.01825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0        1        2        3        4        5        6        7     \\\n",
       "0  0.02500  0.02500  0.05775  0.09675  0.14675  0.21700  0.06700  0.26275   \n",
       "1  0.03975  0.04375  0.03775  0.04800  0.11025  0.07250  0.04775  0.10975   \n",
       "2  0.04350  0.05475  0.02800  0.07700  0.05400  0.06850  0.05600  0.18500   \n",
       "3  0.03725  0.03950  0.03675  0.03100  0.06825  0.04050  0.02325  0.08700   \n",
       "4  0.04200  0.05275  0.05925  0.03675  0.07525  0.12525  0.02850  0.08500   \n",
       "\n",
       "      8        9      ...        1118     1119     1120     1121     1122  \\\n",
       "0  0.26200  0.03200   ...     0.03950  0.01800  0.04575  0.03275  0.12500   \n",
       "1  0.09925  0.02050   ...     0.04175  0.01925  0.01725  0.02425  0.12550   \n",
       "2  0.04925  0.02675   ...     0.04150  0.02675  0.02775  0.03425  0.15550   \n",
       "3  0.05125  0.03025   ...     0.05750  0.03375  0.02275  0.03975  0.18525   \n",
       "4  0.02950  0.02875   ...     0.04250  0.02825  0.02150  0.02600  0.14275   \n",
       "\n",
       "      1123     1124     1125     1126     1127  \n",
       "0  0.04150  0.01925  0.03625  0.07775  0.02300  \n",
       "1  0.02250  0.01550  0.01475  0.09025  0.01875  \n",
       "2  0.03675  0.01700  0.01950  0.09700  0.01850  \n",
       "3  0.05925  0.01500  0.01525  0.06450  0.01300  \n",
       "4  0.02075  0.01650  0.01675  0.10750  0.01825  \n",
       "\n",
       "[5 rows x 1128 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data into Training, Validation and Test sets\n",
    "X_train, X_val, X_test = np.split(X, [int(.8 * len(X)), int(.9 * len(X))])\n",
    "y_train, y_val, y_test = np.split(y, [int(.8 * len(y)), int(.9 * len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1,3,7,15,31,63]\n",
    "validation_accuracies = []\n",
    "for k in k_values:\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    y_val_pred = neigh.predict(X_val)\n",
    "    validation_accuracies.append(accuracy_score(y_val, y_val_pred) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy % for each value of K:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Validation Accuracy %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>83.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>86.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>86.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>86.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>86.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>63</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    K  Validation Accuracy %\n",
       "0   1                   83.6\n",
       "1   3                   86.3\n",
       "2   7                   86.2\n",
       "3  15                   86.7\n",
       "4  31                   86.4\n",
       "5  63                   86.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(k_values, validation_accuracies, 'bo-')\n",
    "plt.xticks(k_values)\n",
    "plt.title(\"Validation Accuracy % vs k\", size=16)\n",
    "plt.xlabel(\"k\", size=14)\n",
    "plt.ylabel(\"Validation Accuracy %\", size=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation Accuracy % for each value of K:\")\n",
    "val_acc_dict = {\"K\": k_values, \"Validation Accuracy %\": validation_accuracies}\n",
    "df_val_acc = pd.DataFrame(val_acc_dict)\n",
    "df_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of K to get highest validation accuracy % = 15\n",
      "Best Validation Accuracy = 86.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"Value of K to get highest validation accuracy % =\", k_values[validation_accuracies.index(max(validation_accuracies))])\n",
    "print(\"Best Validation Accuracy = {}%\".format(max(validation_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 86.7%\n"
     ]
    }
   ],
   "source": [
    "# Using the best value of k\n",
    "neigh = KNeighborsClassifier(n_neighbors=15)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_test_pred = neigh.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test accuracy = {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 15 is the best value as it gives us maximum validation accuracy of 86.7%.\n",
    "The test accuracy is 86.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) State why using an even value of k in k-NN should not be chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide the class that a point belongs to, we calculate its distance from all the other points. Then we choose the \"k\" closest points, and whichever class **majority** of those points belong to, we assign the query point to that **majority class**. If k is even, then we might have a case where half of the k points belong to one class and the other half to the alternate class. Thus we will not have any clear majority, making it impossible to classify the query point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Naive Bayes' classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Continuous Distribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the distribution of the data( $X$ represents the datapoints and $Y$ represents the 0-1 binary-class label; where 0 being the negative class and 1 being the positive class) is already known.\n",
    "<br>Consider the following one-dimensional(1-D) Gaussian distributions where means and variances are unknown. You need to estimate means($\\mu_-$: for negative class and  $\\mu_+$: for positive class) and variances ($\\sigma^{2}_{-}$: for negative class and $\\sigma^{2}_+$: for positive class) from the given data : \n",
    "<br> (1) Assume $X|Y_{Y=0} \\sim \\mathcal{N}(\\mu_- , \\sigma^{2}_-)$ \n",
    "<br>(2) Assume $X|Y_{Y=1} \\sim \\mathcal{N}(\\mu_+ , \\sigma^{2}_+)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generating artificial datasets in the next cell *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is for generating datasets. Students should not change anything in this cell. \n",
    "## You can compare your mean and variance estimates by the actual ones used to generate these datasets\n",
    "\n",
    "import numpy as np\n",
    "X_pos = np.random.randn(1000,1)+np.array([[2.]])\n",
    "X_neg = np.random.randn(1000,1)+np.array([[4.]])\n",
    "X_train_pos = X_pos[:900]\n",
    "X_train_neg = X_neg[:900]\n",
    "X_test_pos = X_pos[900:]\n",
    "X_test_neg = X_neg[900:]\n",
    "X_train = np.concatenate((X_train_pos, X_train_neg), axis=0)\n",
    "X_test = np.concatenate((X_test_pos, X_test_neg), axis=0)\n",
    "Y_train = np.concatenate(( np.ones(900),np.zeros(900) ))\n",
    "Y_test = np.concatenate(( np.ones(100), np.zeros(100) ))\n",
    "\n",
    "## X_train, X_test, Y_train, Y_test are your datasets to work with ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>**Instructions to follow for learning a Baeysian classifier:** *(Code the formulae for estimating the different parameters yourself)*\n",
    "<br> a)Utilize the training dataset to estimate the means($\\hat{\\mu_+}$,$\\hat{\\mu_-}$) and variances($\\hat{\\sigma^{2}_+}$, $\\hat{\\sigma^{2}_-}$) for both positive and negative classes  \n",
    "b)Estimate the prior probability: $P(Y=1)$  ⟶ which could be referred to as: $\\hat{a}$ \n",
    "<br>c)Estimate the classifier funtion/posterior probability:  $P(Y=1|X = x)$  ⟶ which could be referred to as $\\hat{\\eta(x)}$\n",
    "<br>d)Find out the threshold value($x^*$) for classification by equating the estimated classifier function($\\hat{\\eta(x)}$)  with threshold probability of 0.5\n",
    "<br>e)Classify the test dataset into the two classes using this threshold value($x^*$) and find out the **accuracy** of the prediction \n",
    "\n",
    "Return back:  $\\hat{\\mu_+}$, $\\hat{\\mu_-}$, $\\hat{\\sigma^{2}_+}$, $\\hat{\\sigma^{2}_-}$, $\\hat{a}$, $x^*$ and accuracy from the code written \n",
    "\n",
    "*Hint: $X|Y_{Y=0} \\sim \\mathcal{N}(\\mu_- , \\sigma^{2}_-)$ implies $P_{X|Y=0} = \\mathcal{N}(\\mu_- , \\sigma^{2}_-) $*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mean of the positive class = 1.9451297933450715\n",
      "Estimated mean of the negative class = 4.017177962001651\n",
      "\n",
      "Estimated variance of the positive class = 1.0462821374263092\n",
      "Estimated variance of the negative class = 1.000497377629145\n",
      "\n",
      "Prior probability of the positive class = 0.5\n",
      "Prior probability of the negative class = 0.5\n",
      "\n",
      "Threshold value = 2.9821909388188352\n",
      "\n",
      "Training accuracy = 84.66666666666667 %\n",
      "Test accuracy = 83.0 %\n"
     ]
    }
   ],
   "source": [
    "# Number of examples in each class\n",
    "num1 = sum(Y_train==np.ones(len(Y_train)))\n",
    "num2 = len(Y_train) - num1\n",
    "\n",
    "# Estimating mean of the positive class\n",
    "sum1 = 0\n",
    "for i, x in enumerate(X_train):\n",
    "    if Y_train[i]==1:\n",
    "        sum1 += x[0]\n",
    "mu1 = sum1/num1\n",
    "print(\"Estimated mean of the positive class =\", mu1)\n",
    "\n",
    "# Estimating mean of the negative class\n",
    "sum2 = 0\n",
    "for i, x in enumerate(X_train):\n",
    "    if Y_train[i]==0:\n",
    "        sum2 += x[0]\n",
    "mu2 = sum2/num2\n",
    "print(\"Estimated mean of the negative class =\", mu2)\n",
    "print()\n",
    "\n",
    "# Estimating variance of the positive class\n",
    "sum1 = 0\n",
    "for i, x in enumerate(X_train):\n",
    "    if Y_train[i]==1:\n",
    "        sum1 += (x[0] - mu1)**2\n",
    "var1 = sum1/(num1-1)\n",
    "print(\"Estimated variance of the positive class =\", var1)\n",
    "\n",
    "# Estimating variance of the negative class\n",
    "sum2 = 0\n",
    "for i, x in enumerate(X_train):\n",
    "    if Y_train[i]==0:\n",
    "        sum2 += (x[0] - mu2)**2\n",
    "var2 = sum2/(num2-1)\n",
    "print(\"Estimated variance of the negative class =\", var2)\n",
    "print()\n",
    "\n",
    "# Estimating prior probabilities\n",
    "Pr1 = num1/len(Y_train)\n",
    "Pr2 = num2/len(Y_train)\n",
    "print(\"Prior probability of the positive class =\", Pr1)\n",
    "print(\"Prior probability of the negative class =\", Pr2)\n",
    "print()\n",
    "\n",
    "def Gaussian(x, mu, var):\n",
    "    return (1/np.sqrt(2*np.pi*var))*np.exp(-1 * ((x-mu)**2)/(2*var))\n",
    "\n",
    "def posterior_prob(x):\n",
    "    return (Pr1*Gaussian(x, mu1, var1))/(Pr1*Gaussian(x, mu1, var1) + Pr2*Gaussian(x, mu2, var2))\n",
    "\n",
    "# Estimate the threshold value\n",
    "x_values = np.linspace(mu1, mu2, 1000)\n",
    "diff = [None]*1000\n",
    "for i, x in enumerate(x_values):\n",
    "    prob = posterior_prob(x)\n",
    "    diff[i] = abs(0.5 - prob)\n",
    "    \n",
    "threshold = x_values[diff.index(min(diff))]\n",
    "print(\"Threshold value = {}\".format(threshold))\n",
    "print()\n",
    "\n",
    "# Classification of the test data\n",
    "y_train_pred = X_train <= threshold\n",
    "train_result = y_train_pred[:, 0]==Y_train\n",
    "train_acc = np.mean(train_result)\n",
    "print(\"Training accuracy = {} %\".format(train_acc*100))\n",
    "\n",
    "y_test_pred = X_test <= threshold\n",
    "test_result = y_test_pred[:, 0]==Y_test\n",
    "test_acc = np.mean(test_result)\n",
    "print(\"Test accuracy = {} %\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. * Estimated mean of the positive class = 1.9565564962996493\n",
    "   * Estimated mean of the negative class = 3.9746368049663316\n",
    "\n",
    "\n",
    "2. * Estimated variance of the positive class = 1.013554256583211\n",
    "   * Estimated variance of the negative class = 1.0268496089892385\n",
    "\n",
    "\n",
    "3. * Prior probability of the positive class = 0.5\n",
    "   * Prior probability of the negative class = 0.5\n",
    "\n",
    "\n",
    "4. Threshold value **x*** = 2.966606700837528\n",
    "Every point to the left of the **x*** is classified as belonging to the positive class (Y=1).\n",
    "\n",
    "\n",
    "5. * Training accuracy = **83.5 %**\n",
    "   * Test accuracy = **85.0 %**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Discrete distribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the first exercise for learning the Naive Bayes' classifier where we dealt with continuous distribution of data, here you need to work with discrete data, which means finding Probability Mass Distribution(PMF). \n",
    "\n",
    "Age  | Income | Status  | Buy\n",
    "-----|--------|-------- |----\n",
    "<=20 |  low   | students| yes\n",
    "<=20 |  high  | students| yes\n",
    "<=20 | medium | students| no\n",
    "<=20 | medium | married | no\n",
    "<=20 |  high  | married | yes\n",
    "21-30|  low   | married | yes\n",
    "21-30|  low   | married | no \n",
    "21-30| medium | students| no\n",
    "21-30|  high  | students| yes\n",
    " >30 |  high  | married | no\n",
    " >30 |  high  | married | yes\n",
    " >30 | medium | married | yes\n",
    " >30 | medium | married | no\n",
    " >30 | medium | students| no\n",
    " \n",
    "Consider the train dataset above. Take any random datapoint ($X_{i}$) where $X_{i} = (X_{i,1} = Age,X_{i,2} = Income,X_{i,3} = Status)$ and its corresponding label \n",
    "\n",
    "($Y_{i} = Buy$). A \"yes\" in Buy corresponds to label-1 and a \"no\" in Buy corresponds to label-0.\n",
    "\n",
    "<br>**Instructions to follow for learning a Baeysian classifier:** *(Code the formulae for estimating the different parameters yourself)*\n",
    "<br> a)Estimate the prior probability: $P(Y=1)$  ⟶ which could be referred to as: $\\hat{a}$   \n",
    "b)Estimate the likelihood for each feature:  $P(X_{i,j} = x |Y = y_{i})$, where $ i$=datapoint counter, $j \\in \\{1,2,3\\}$ and $y_{i} \\in \\{0,1\\}$ \n",
    "<br>c)Estimate the total likelihood: $P(X_{i} = x |Y = y_{i})$  \n",
    "d)Calculate the posterior probability: $P(Y = 1|X_{i} = x_{test} )$ = $p_{test}$ where $x_{test} = (Age = 21-30, Income= medium, Status = married)$\n",
    "\n",
    "\n",
    "Return back: $\\hat{a}$, total likelihood and $p_{test}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "Age  | Income | Status  | Buy\n",
    "-----|--------|-------- |----\n",
    "<=20 |  low   | students| yes\n",
    "<=20 |  high  | students| yes\n",
    "<=20 | medium | students| no\n",
    "<=20 | medium | married | no\n",
    "<=20 |  high  | married | yes\n",
    "21-30|  low   | married | yes\n",
    "21-30|  low   | married | no \n",
    "21-30| medium | students| no\n",
    "21-30|  high  | students| yes\n",
    " >30 |  high  | married | no\n",
    " >30 |  high  | married | yes\n",
    " >30 | medium | married | yes\n",
    " >30 | medium | married | no\n",
    " >30 | medium | students| no\"\"\"\n",
    "data = data.split(\"\\n\")[1:]\n",
    "age = []\n",
    "income = []\n",
    "status = []\n",
    "buy = []\n",
    "for i in range(2, len(data)):\n",
    "    a, b, c, d = [entry.strip() for entry in data[i].split(\"|\")]\n",
    "    age.append(a)\n",
    "    income.append(b)\n",
    "    status.append(c)\n",
    "    buy.append(d)\n",
    "    \n",
    "features = [age, income, status]\n",
    "table = {\"age\": age, \"income\": income, \"status\": status}\n",
    "n = len(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probability P(Y = 1) = 0.5\n"
     ]
    }
   ],
   "source": [
    "## write your code here.\n",
    "\n",
    "# Prior probability P(Y = 1)\n",
    "Pr = buy.count(\"yes\")/n\n",
    "print(\"Prior Probability P(Y = 1) =\", Pr)\n",
    "\n",
    "pos_indices = []\n",
    "neg_indices = []\n",
    "for k in range(n):\n",
    "    if buy[k]=='yes':\n",
    "        pos_indices.append(k)\n",
    "    else:\n",
    "        neg_indices.append(k)\n",
    "\n",
    "# Likelihood for each feature\n",
    "P = {\"yes\":{}, \"no\":{}}\n",
    "for key in table:\n",
    "    unique_vals = set(table[key])\n",
    "    P[\"yes\"][key] = {}\n",
    "    P[\"no\"][key] = {}\n",
    "    for val in unique_vals:\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        for k in range(n):\n",
    "            if buy[k]=='yes' and table[key][k]==val:\n",
    "                pos_count += 1\n",
    "            if buy[k]=='no' and table[key][k]==val:\n",
    "                neg_count += 1\n",
    "        P[\"yes\"][key][val] = pos_count/len(pos_indices)\n",
    "        P[\"no\"][key][val] = neg_count/len(neg_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Conditional Probabilities for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X age = <=20 | Y = yes) = 0.42857142857142855\n",
      "P(X age = 21-30 | Y = yes) = 0.2857142857142857\n",
      "P(X age = >30 | Y = yes) = 0.2857142857142857\n",
      "\n",
      "P(X income = low | Y = yes) = 0.2857142857142857\n",
      "P(X income = high | Y = yes) = 0.5714285714285714\n",
      "P(X income = medium | Y = yes) = 0.14285714285714285\n",
      "\n",
      "P(X status = married | Y = yes) = 0.5714285714285714\n",
      "P(X status = students | Y = yes) = 0.42857142857142855\n",
      "\n",
      "P(X age = <=20 | Y = no) = 0.2857142857142857\n",
      "P(X age = 21-30 | Y = no) = 0.2857142857142857\n",
      "P(X age = >30 | Y = no) = 0.42857142857142855\n",
      "\n",
      "P(X income = low | Y = no) = 0.14285714285714285\n",
      "P(X income = high | Y = no) = 0.14285714285714285\n",
      "P(X income = medium | Y = no) = 0.7142857142857143\n",
      "\n",
      "P(X status = married | Y = no) = 0.5714285714285714\n",
      "P(X status = students | Y = no) = 0.42857142857142855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in P:\n",
    "    for key2 in P[key]:\n",
    "        for val in P[key][key2]:\n",
    "            print(\"P(X {} = {} | Y = {}) = {}\".format(key2, val, key, P[key][key2][val]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = 1 case\n",
      "P(X age = <=20, income = low, status = married | Y = 1) = 0.06997084548104955\n",
      "P(X age = <=20, income = low, status = students | Y = 1) = 0.05247813411078716\n",
      "P(X age = <=20, income = high, status = married | Y = 1) = 0.1399416909620991\n",
      "P(X age = <=20, income = high, status = students | Y = 1) = 0.10495626822157432\n",
      "P(X age = <=20, income = medium, status = married | Y = 1) = 0.034985422740524776\n",
      "P(X age = <=20, income = medium, status = students | Y = 1) = 0.02623906705539358\n",
      "P(X age = 21-30, income = low, status = married | Y = 1) = 0.0466472303206997\n",
      "P(X age = 21-30, income = low, status = students | Y = 1) = 0.034985422740524776\n",
      "P(X age = 21-30, income = high, status = married | Y = 1) = 0.0932944606413994\n",
      "P(X age = 21-30, income = high, status = students | Y = 1) = 0.06997084548104955\n",
      "P(X age = 21-30, income = medium, status = married | Y = 1) = 0.02332361516034985\n",
      "P(X age = 21-30, income = medium, status = students | Y = 1) = 0.017492711370262388\n",
      "P(X age = >30, income = low, status = married | Y = 1) = 0.0466472303206997\n",
      "P(X age = >30, income = low, status = students | Y = 1) = 0.034985422740524776\n",
      "P(X age = >30, income = high, status = married | Y = 1) = 0.0932944606413994\n",
      "P(X age = >30, income = high, status = students | Y = 1) = 0.06997084548104955\n",
      "P(X age = >30, income = medium, status = married | Y = 1) = 0.02332361516034985\n",
      "P(X age = >30, income = medium, status = students | Y = 1) = 0.017492711370262388\n",
      "\n",
      "Y = 0 case\n",
      "P(X age = <=20, income = low, status = married | Y = 0) = 0.02332361516034985\n",
      "P(X age = <=20, income = low, status = students | Y = 0) = 0.017492711370262388\n",
      "P(X age = <=20, income = high, status = married | Y = 0) = 0.02332361516034985\n",
      "P(X age = <=20, income = high, status = students | Y = 0) = 0.017492711370262388\n",
      "P(X age = <=20, income = medium, status = married | Y = 0) = 0.11661807580174927\n",
      "P(X age = <=20, income = medium, status = students | Y = 0) = 0.08746355685131195\n",
      "P(X age = 21-30, income = low, status = married | Y = 0) = 0.02332361516034985\n",
      "P(X age = 21-30, income = low, status = students | Y = 0) = 0.017492711370262388\n",
      "P(X age = 21-30, income = high, status = married | Y = 0) = 0.02332361516034985\n",
      "P(X age = 21-30, income = high, status = students | Y = 0) = 0.017492711370262388\n",
      "P(X age = 21-30, income = medium, status = married | Y = 0) = 0.11661807580174927\n",
      "P(X age = 21-30, income = medium, status = students | Y = 0) = 0.08746355685131195\n",
      "P(X age = >30, income = low, status = married | Y = 0) = 0.034985422740524776\n",
      "P(X age = >30, income = low, status = students | Y = 0) = 0.02623906705539358\n",
      "P(X age = >30, income = high, status = married | Y = 0) = 0.034985422740524776\n",
      "P(X age = >30, income = high, status = students | Y = 0) = 0.02623906705539358\n",
      "P(X age = >30, income = medium, status = married | Y = 0) = 0.1749271137026239\n",
      "P(X age = >30, income = medium, status = students | Y = 0) = 0.13119533527696792\n"
     ]
    }
   ],
   "source": [
    "age_vals = set(list(age))\n",
    "income_vals = set(list(income))\n",
    "status_vals = set(list(status))\n",
    "\n",
    "print(\"Y = 1 case\")\n",
    "for a in age_vals:\n",
    "    for b in income_vals:\n",
    "        for c in status_vals:\n",
    "            answer = P[\"yes\"][\"age\"][a] * P[\"yes\"][\"income\"][b] * P[\"yes\"][\"status\"][c]\n",
    "            print(\"P(X age = {}, income = {}, status = {} | Y = 1) = {}\".format(a, b, c, answer))\n",
    "        \n",
    "print()\n",
    "print(\"Y = 0 case\")\n",
    "for a in age_vals:\n",
    "    for b in income_vals:\n",
    "        for c in status_vals:\n",
    "            answer = P[\"no\"][\"age\"][a] * P[\"no\"][\"income\"][b] * P[\"no\"][\"status\"][c]\n",
    "            print(\"P(X age = {}, income = {}, status = {} | Y = 0) = {}\".format(a, b, c, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Y = 1 | xtest = Age=21-30,Income=medium,Status=married) = 0.16666666666666663\n",
      "P(Y = 0 | xtest = Age=21-30,Income=medium,Status=married) = 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "x_age = \"21-30\"\n",
    "x_income = \"medium\"\n",
    "x_status = \"married\"\n",
    "class_conditional = P[\"yes\"][\"age\"][x_age] * P[\"yes\"][\"income\"][x_income] * P[\"yes\"][\"status\"][x_status]\n",
    "class_conditional_opp = P[\"no\"][\"age\"][x_age] * P[\"no\"][\"income\"][x_income] * P[\"no\"][\"status\"][x_status]\n",
    "\n",
    "req_probability = (class_conditional * Pr)/((class_conditional * Pr) + (class_conditional_opp * (1 - Pr)))\n",
    "print(\"P(Y = 1 | xtest = Age={},Income={},Status={}) = {}\".format(x_age, x_income, x_status, req_probability))\n",
    "print(\"P(Y = 0 | xtest = Age={},Income={},Status={}) = {}\".format(x_age, x_income, x_status, 1-req_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior probability for the x_test to lie in the Y = 1 class is 0.16663, the posterior probability for the x_test to lie in the Y = 0 class is 0.8333333333333334. Therefore, the classifier will predict that the x_test lies in the Y = 0 class. The classifier will predict that the x_test belongs to the negative class (\"no\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
